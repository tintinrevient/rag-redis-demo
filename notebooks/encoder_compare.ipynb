{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240ed88c-7bc5-4894-b27f-f640c85df8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nezumikozo/Documents/workspace/rag-redis-demo/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, CanineTokenizer, CanineModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from my_util import get_chunks, get_topk_similarity, show_tokens, get_batched_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606e225-6681-49b5-8a4a-8c501d3efa31",
   "metadata": {},
   "source": [
    "## Test embedders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb78221-e7c1-4648-b5d1-58bea00f040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_query = \"what are scope 1 emissions?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e4fd3a-013a-4e3a-8081-e83615165eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "content, metadata = get_chunks(company_name=\"novo_nordisk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2cd0a1-0f0e-4bfb-8f45-46fc2baad115",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeders = [\n",
    "    SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v4\", cache_folder=\"cache\"),\n",
    "    SentenceTransformer('sentence-transformers/all-mpnet-base-v2', cache_folder=\"cache\"),\n",
    "    SentenceTransformer(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\", cache_folder=\"cache\"),\n",
    "    SentenceTransformer(\"sentence-transformers/multi-qa-distilbert-cos-v1\", cache_folder=\"cache\"),\n",
    "    SentenceTransformer('sentence-transformers/all-distilroberta-v1', cache_folder=\"cache\"),\n",
    "    SentenceTransformer(\"sentence-transformers/msmarco-distilbert-dot-v5\", cache_folder=\"cache\"),\n",
    "    SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-tas-b\", cache_folder=\"cache\"),\n",
    "    SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", cache_folder=\"cache\"),  # Default for RAG Redis\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59360eea-e988-4e8a-9a21-107c30c016ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n",
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "260 \t 0.5144\n",
      "261 \t 0.4494\n",
      "19 \t 0.4473\n",
      "21 \t 0.4090\n",
      "20 \t 0.3557\n",
      "259 \t 0.3184\n",
      "306 \t 0.3183\n",
      "22 \t 0.3165\n",
      "258 \t 0.2938\n",
      "52 \t 0.2917\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ")\n",
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "260 \t 0.3954\n",
      "263 \t 0.3939\n",
      "306 \t 0.3568\n",
      "261 \t 0.3469\n",
      "265 \t 0.3414\n",
      "259 \t 0.3407\n",
      "262 \t 0.3368\n",
      "20 \t 0.3340\n",
      "55 \t 0.2738\n",
      "307 \t 0.2691\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n",
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "260 \t 0.5712\n",
      "23 \t 0.5061\n",
      "261 \t 0.4998\n",
      "20 \t 0.4866\n",
      "263 \t 0.4737\n",
      "259 \t 0.4491\n",
      "265 \t 0.4406\n",
      "306 \t 0.4331\n",
      "262 \t 0.4321\n",
      "19 \t 0.4212\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ")\n",
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "261 \t 0.5716\n",
      "260 \t 0.5483\n",
      "23 \t 0.4769\n",
      "22 \t 0.4493\n",
      "262 \t 0.4480\n",
      "20 \t 0.4435\n",
      "21 \t 0.4382\n",
      "19 \t 0.4372\n",
      "266 \t 0.4313\n",
      "258 \t 0.3917\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ")\n",
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "262 \t 0.5378\n",
      "19 \t 0.5120\n",
      "265 \t 0.5017\n",
      "23 \t 0.4937\n",
      "264 \t 0.4878\n",
      "20 \t 0.4845\n",
      "261 \t 0.4788\n",
      "258 \t 0.4777\n",
      "22 \t 0.4766\n",
      "260 \t 0.4761\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n",
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "260 \t 0.7606\n",
      "262 \t 0.7580\n",
      "261 \t 0.7520\n",
      "19 \t 0.7308\n",
      "265 \t 0.7198\n",
      "264 \t 0.7184\n",
      "23 \t 0.7153\n",
      "266 \t 0.6953\n",
      "21 \t 0.6949\n",
      "22 \t 0.6918\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n",
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "260 \t 0.8382\n",
      "262 \t 0.8159\n",
      "261 \t 0.8079\n",
      "259 \t 0.7817\n",
      "19 \t 0.7793\n",
      "265 \t 0.7757\n",
      "23 \t 0.7705\n",
      "22 \t 0.7657\n",
      "21 \t 0.7652\n",
      "266 \t 0.7557\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ")\n",
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "262 \t 0.5487\n",
      "260 \t 0.5242\n",
      "22 \t 0.5236\n",
      "261 \t 0.4791\n",
      "20 \t 0.4449\n",
      "306 \t 0.4362\n",
      "259 \t 0.4360\n",
      "19 \t 0.4331\n",
      "263 \t 0.4311\n",
      "21 \t 0.4241\n"
     ]
    }
   ],
   "source": [
    "for test_embeder in test_embeders:\n",
    "    print(test_embeder)\n",
    "\n",
    "    embedded_query = test_embeder.encode(default_query)\n",
    "    embedded_content = test_embeder.encode(content)\n",
    "\n",
    "    get_topk_similarity(\n",
    "        k=10, \n",
    "        encoded_query=embedded_query, \n",
    "        encoded_docs=embedded_content, \n",
    "        is_cos_sim=True, \n",
    "        debug=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0737241-3abd-4cf7-8276-33ee865dcdd6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: honeydew\">\n",
    "\n",
    "## Why do embedders perform differently?\n",
    "For how to recognize words and build vocabulary:\n",
    "- Tokenization methods?\n",
    "- Training data for tokenizers?\n",
    "\n",
    "For context to embed meanings:\n",
    "- Encoder architecture and params?\n",
    "- Training data for encoders?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5d1f9-205c-48b0-9525-834966ab3215",
   "metadata": {},
   "source": [
    "## Focus on tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd59fd5-fcf4-4e51-a6d1-1748a989264f",
   "metadata": {},
   "source": [
    "`Word tokens`: This approach was common with earlier methods like `Word2Vec` but is being used less and less in NLP.\n",
    "* One challenge with word tokenization is that the tokenizer is unable to deal with new words that enter the dataset.\n",
    "* It also results in a vocabulary that has a lot of tokens with minimal differences between them (e.g., apology, apologize, apologetic, apologist).\n",
    "\n",
    "`Subword tokens`: This method contains full and partial words. \n",
    "* In addition to the vocabulary diversity, another benefit of this approach is its ability to represent new words by breaking the new token down into smaller characters, which tend to be a part of the vocabulary.\n",
    "* When compared to character tokens, this method benefits from the ability to fit more text within the limited context length of a Transformer model.\n",
    "\n",
    "`Character tokens`: This is another method that is able to deal successfully with new words because it has the raw letters to fall-back on.\n",
    "\n",
    "`Byte tokens`: One additional tokenization method breaks down tokens into the individual bytes that are used to represent unicode characters, such as `BPE (Byte-Pair Encoding)` is widely used by GPT models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f754bb52-5926-48ad-911e-b16073c63358",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"\n",
    "In 2022, Scope 1 emissions decreased by 1% compared to 2021 due to an increase \n",
    "in usage of renewable energy sources from 7.4 to 7.3 (1,000 tonnes CO2), and \n",
    "production sites consumed 3,918 thousand cubic metres of water less than last year's 1,345,340.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ab3e39-a49c-4fa2-8d18-dba9286c2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-tas-b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57b350e-c2c9-49c1-8917-8c19d3388111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 68\n",
      "tensor(101) -> [CLS]\n",
      "tensor(1999) -> in\n",
      "tensor(16798) -> 202\n",
      "tensor(2475) -> ##2\n",
      "tensor(1010) -> ,\n",
      "tensor(9531) -> scope\n",
      "tensor(1015) -> 1\n",
      "tensor(11768) -> emissions\n",
      "tensor(10548) -> decreased\n",
      "tensor(2011) -> by\n",
      "tensor(1015) -> 1\n",
      "tensor(1003) -> %\n",
      "tensor(4102) -> compared\n",
      "tensor(2000) -> to\n",
      "tensor(25682) -> 2021\n",
      "tensor(2349) -> due\n",
      "tensor(2000) -> to\n",
      "tensor(2019) -> an\n",
      "tensor(3623) -> increase\n",
      "tensor(1999) -> in\n",
      "tensor(8192) -> usage\n",
      "tensor(1997) -> of\n",
      "tensor(13918) -> renewable\n",
      "tensor(2943) -> energy\n",
      "tensor(4216) -> sources\n",
      "tensor(2013) -> from\n",
      "tensor(1021) -> 7\n",
      "tensor(1012) -> .\n",
      "tensor(1018) -> 4\n",
      "tensor(2000) -> to\n",
      "tensor(1021) -> 7\n",
      "tensor(1012) -> .\n",
      "tensor(1017) -> 3\n",
      "tensor(1006) -> (\n",
      "tensor(1015) -> 1\n",
      "tensor(1010) -> ,\n",
      "tensor(2199) -> 000\n",
      "tensor(11000) -> tonnes\n",
      "tensor(2522) -> co\n",
      "tensor(2475) -> ##2\n",
      "tensor(1007) -> )\n",
      "tensor(1010) -> ,\n",
      "tensor(1998) -> and\n",
      "tensor(2537) -> production\n",
      "tensor(4573) -> sites\n",
      "tensor(10202) -> consumed\n",
      "tensor(1017) -> 3\n",
      "tensor(1010) -> ,\n",
      "tensor(6205) -> 91\n",
      "tensor(2620) -> ##8\n",
      "tensor(4595) -> thousand\n",
      "tensor(11919) -> cubic\n",
      "tensor(3620) -> metres\n",
      "tensor(1997) -> of\n",
      "tensor(2300) -> water\n",
      "tensor(2625) -> less\n",
      "tensor(2084) -> than\n",
      "tensor(2197) -> last\n",
      "tensor(2095) -> year\n",
      "tensor(1005) -> '\n",
      "tensor(1055) -> s\n",
      "tensor(1015) -> 1\n",
      "tensor(1010) -> ,\n",
      "tensor(23785) -> 345\n",
      "tensor(1010) -> ,\n",
      "tensor(16029) -> 340\n",
      "tensor(1012) -> .\n",
      "tensor(102) -> [SEP]\n"
     ]
    }
   ],
   "source": [
    "show_tokens(sentence=test_sentence, tokenizer=subword_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d71414-66a1-4e04-ab42-f69482ec72e8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: honeydew\">\n",
    "\n",
    "### Questions\n",
    "- Do 1s in \"1%\" and \"scope 1\" have different embeddings, though they have the same token ID?\n",
    "- Each number is a new word? How about number should be treated as composed of digit (character)?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c73d903-17a0-482f-ad56-d3d9b800c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ddf9f62-ae9d-4b1f-9b00-155e8993b764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 256\n",
      "tensor(13) -> \n",
      "\n",
      "tensor(76) -> I\n",
      "tensor(113) -> n\n",
      "tensor(35) ->  \n",
      "tensor(53) -> 2\n",
      "tensor(51) -> 0\n",
      "tensor(53) -> 2\n",
      "tensor(53) -> 2\n",
      "tensor(47) -> ,\n",
      "tensor(35) ->  \n",
      "tensor(86) -> S\n",
      "tensor(102) -> c\n",
      "tensor(114) -> o\n",
      "tensor(115) -> p\n",
      "tensor(104) -> e\n",
      "tensor(35) ->  \n",
      "tensor(52) -> 1\n",
      "tensor(35) ->  \n",
      "tensor(104) -> e\n",
      "tensor(112) -> m\n",
      "tensor(108) -> i\n",
      "tensor(118) -> s\n",
      "tensor(118) -> s\n",
      "tensor(108) -> i\n",
      "tensor(114) -> o\n",
      "tensor(113) -> n\n",
      "tensor(118) -> s\n",
      "tensor(35) ->  \n",
      "tensor(103) -> d\n",
      "tensor(104) -> e\n",
      "tensor(102) -> c\n",
      "tensor(117) -> r\n",
      "tensor(104) -> e\n",
      "tensor(100) -> a\n",
      "tensor(118) -> s\n",
      "tensor(104) -> e\n",
      "tensor(103) -> d\n",
      "tensor(35) ->  \n",
      "tensor(101) -> b\n",
      "tensor(124) -> y\n",
      "tensor(35) ->  \n",
      "tensor(52) -> 1\n",
      "tensor(40) -> %\n",
      "tensor(35) ->  \n",
      "tensor(102) -> c\n",
      "tensor(114) -> o\n",
      "tensor(112) -> m\n",
      "tensor(115) -> p\n",
      "tensor(100) -> a\n",
      "tensor(117) -> r\n",
      "tensor(104) -> e\n",
      "tensor(103) -> d\n",
      "tensor(35) ->  \n",
      "tensor(119) -> t\n",
      "tensor(114) -> o\n",
      "tensor(35) ->  \n",
      "tensor(53) -> 2\n",
      "tensor(51) -> 0\n",
      "tensor(53) -> 2\n",
      "tensor(52) -> 1\n",
      "tensor(35) ->  \n",
      "tensor(103) -> d\n",
      "tensor(120) -> u\n",
      "tensor(104) -> e\n",
      "tensor(35) ->  \n",
      "tensor(119) -> t\n",
      "tensor(114) -> o\n",
      "tensor(35) ->  \n",
      "tensor(100) -> a\n",
      "tensor(113) -> n\n",
      "tensor(35) ->  \n",
      "tensor(108) -> i\n",
      "tensor(113) -> n\n",
      "tensor(102) -> c\n",
      "tensor(117) -> r\n",
      "tensor(104) -> e\n",
      "tensor(100) -> a\n",
      "tensor(118) -> s\n",
      "tensor(104) -> e\n",
      "tensor(35) ->  \n",
      "tensor(13) -> \n",
      "\n",
      "tensor(108) -> i\n",
      "tensor(113) -> n\n",
      "tensor(35) ->  \n",
      "tensor(120) -> u\n",
      "tensor(118) -> s\n",
      "tensor(100) -> a\n",
      "tensor(106) -> g\n",
      "tensor(104) -> e\n",
      "tensor(35) ->  \n",
      "tensor(114) -> o\n",
      "tensor(105) -> f\n",
      "tensor(35) ->  \n",
      "tensor(117) -> r\n",
      "tensor(104) -> e\n",
      "tensor(113) -> n\n",
      "tensor(104) -> e\n",
      "tensor(122) -> w\n",
      "tensor(100) -> a\n",
      "tensor(101) -> b\n",
      "tensor(111) -> l\n",
      "tensor(104) -> e\n",
      "tensor(35) ->  \n",
      "tensor(104) -> e\n",
      "tensor(113) -> n\n",
      "tensor(104) -> e\n",
      "tensor(117) -> r\n",
      "tensor(106) -> g\n",
      "tensor(124) -> y\n",
      "tensor(35) ->  \n",
      "tensor(118) -> s\n",
      "tensor(114) -> o\n",
      "tensor(120) -> u\n",
      "tensor(117) -> r\n",
      "tensor(102) -> c\n",
      "tensor(104) -> e\n",
      "tensor(118) -> s\n",
      "tensor(35) ->  \n",
      "tensor(105) -> f\n",
      "tensor(117) -> r\n",
      "tensor(114) -> o\n",
      "tensor(112) -> m\n",
      "tensor(35) ->  \n",
      "tensor(58) -> 7\n",
      "tensor(49) -> .\n",
      "tensor(55) -> 4\n",
      "tensor(35) ->  \n",
      "tensor(119) -> t\n",
      "tensor(114) -> o\n",
      "tensor(35) ->  \n",
      "tensor(58) -> 7\n",
      "tensor(49) -> .\n",
      "tensor(54) -> 3\n",
      "tensor(35) ->  \n",
      "tensor(43) -> (\n",
      "tensor(52) -> 1\n",
      "tensor(47) -> ,\n",
      "tensor(51) -> 0\n",
      "tensor(51) -> 0\n",
      "tensor(51) -> 0\n",
      "tensor(35) ->  \n",
      "tensor(119) -> t\n",
      "tensor(114) -> o\n",
      "tensor(113) -> n\n",
      "tensor(113) -> n\n",
      "tensor(104) -> e\n",
      "tensor(118) -> s\n",
      "tensor(35) ->  \n",
      "tensor(70) -> C\n",
      "tensor(82) -> O\n",
      "tensor(53) -> 2\n",
      "tensor(44) -> )\n",
      "tensor(47) -> ,\n",
      "tensor(35) ->  \n",
      "tensor(100) -> a\n",
      "tensor(113) -> n\n",
      "tensor(103) -> d\n",
      "tensor(35) ->  \n",
      "tensor(13) -> \n",
      "\n",
      "tensor(115) -> p\n",
      "tensor(117) -> r\n",
      "tensor(114) -> o\n",
      "tensor(103) -> d\n",
      "tensor(120) -> u\n",
      "tensor(102) -> c\n",
      "tensor(119) -> t\n",
      "tensor(108) -> i\n",
      "tensor(114) -> o\n",
      "tensor(113) -> n\n",
      "tensor(35) ->  \n",
      "tensor(118) -> s\n",
      "tensor(108) -> i\n",
      "tensor(119) -> t\n",
      "tensor(104) -> e\n",
      "tensor(118) -> s\n",
      "tensor(35) ->  \n",
      "tensor(102) -> c\n",
      "tensor(114) -> o\n",
      "tensor(113) -> n\n",
      "tensor(118) -> s\n",
      "tensor(120) -> u\n",
      "tensor(112) -> m\n",
      "tensor(104) -> e\n",
      "tensor(103) -> d\n",
      "tensor(35) ->  \n",
      "tensor(54) -> 3\n",
      "tensor(47) -> ,\n",
      "tensor(60) -> 9\n",
      "tensor(52) -> 1\n",
      "tensor(59) -> 8\n",
      "tensor(35) ->  \n",
      "tensor(119) -> t\n",
      "tensor(107) -> h\n",
      "tensor(114) -> o\n",
      "tensor(120) -> u\n",
      "tensor(118) -> s\n",
      "tensor(100) -> a\n",
      "tensor(113) -> n\n",
      "tensor(103) -> d\n",
      "tensor(35) ->  \n",
      "tensor(102) -> c\n",
      "tensor(120) -> u\n",
      "tensor(101) -> b\n",
      "tensor(108) -> i\n",
      "tensor(102) -> c\n",
      "tensor(35) ->  \n",
      "tensor(112) -> m\n",
      "tensor(104) -> e\n",
      "tensor(119) -> t\n",
      "tensor(117) -> r\n",
      "tensor(104) -> e\n",
      "tensor(118) -> s\n",
      "tensor(35) ->  \n",
      "tensor(114) -> o\n",
      "tensor(105) -> f\n",
      "tensor(35) ->  \n",
      "tensor(122) -> w\n",
      "tensor(100) -> a\n",
      "tensor(119) -> t\n",
      "tensor(104) -> e\n",
      "tensor(117) -> r\n",
      "tensor(35) ->  \n",
      "tensor(111) -> l\n",
      "tensor(104) -> e\n",
      "tensor(118) -> s\n",
      "tensor(118) -> s\n",
      "tensor(35) ->  \n",
      "tensor(119) -> t\n",
      "tensor(107) -> h\n",
      "tensor(100) -> a\n",
      "tensor(113) -> n\n",
      "tensor(35) ->  \n",
      "tensor(111) -> l\n",
      "tensor(100) -> a\n",
      "tensor(118) -> s\n",
      "tensor(119) -> t\n",
      "tensor(35) ->  \n",
      "tensor(124) -> y\n",
      "tensor(104) -> e\n",
      "tensor(100) -> a\n",
      "tensor(117) -> r\n",
      "tensor(42) -> '\n",
      "tensor(118) -> s\n",
      "tensor(35) ->  \n",
      "tensor(52) -> 1\n",
      "tensor(47) -> ,\n",
      "tensor(54) -> 3\n",
      "tensor(55) -> 4\n",
      "tensor(56) -> 5\n",
      "tensor(47) -> ,\n",
      "tensor(54) -> 3\n",
      "tensor(55) -> 4\n",
      "tensor(51) -> 0\n",
      "tensor(49) -> .\n",
      "tensor(13) -> \n",
      "\n",
      "tensor(1) -> </s>\n"
     ]
    }
   ],
   "source": [
    "show_tokens(sentence=test_sentence, tokenizer=char_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb93a6-e604-406f-894c-fe25a871c07c",
   "metadata": {},
   "source": [
    "## If character tokenizers perform better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178e8da3-a4a4-42ea-a651-9701e546207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CanineModel.from_pretrained(\"google/canine-c\")\n",
    "tokenizer = CanineTokenizer.from_pretrained(\"google/canine-c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf07c34b-8d25-4ac0-a990-b630615004c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▊                                                                                                                                  | 1/28 [00:27<12:12, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedded tokens: torch.Size([12, 1522, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [07:55<00:00, 16.98s/it]\n"
     ]
    }
   ],
   "source": [
    "embedded_docs = get_batched_embeddings(\n",
    "    sentences=content, \n",
    "    batch_size=12, \n",
    "    tokenizer=tokenizer, \n",
    "    model=model, \n",
    "    padding=\"longest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ddef0fa-f2f0-4089-b1e4-39f864e0b66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedded tokens: torch.Size([1, 29, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedded_query = get_batched_embeddings(\n",
    "    sentences=[default_query], \n",
    "    batch_size=12, \n",
    "    tokenizer=tokenizer, \n",
    "    model=model, \n",
    "    padding=\"longest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "493bc468-445a-4e6e-af3c-65e2037004e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar pairs:\n",
      "doc_idx\t score\n",
      "290 \t 0.6896\n",
      "297 \t 0.6896\n",
      "176 \t 0.6860\n",
      "288 \t 0.6839\n",
      "264 \t 0.6829\n",
      "291 \t 0.6801\n",
      "137 \t 0.6748\n",
      "308 \t 0.6743\n",
      "267 \t 0.6715\n",
      "207 \t 0.6697\n"
     ]
    }
   ],
   "source": [
    "get_topk_similarity(\n",
    "    k=10, \n",
    "    encoded_query=embedded_query, \n",
    "    encoded_docs=embedded_docs, \n",
    "    is_cos_sim=True, \n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c127826-9475-407a-af4d-e04eac0be938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
